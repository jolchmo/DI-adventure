diff --git a/mario_dqn/README.md b/mario_dqn/README.md
index 6b8ff65..6d0c69d 100644
--- a/mario_dqn/README.md
+++ b/mario_dqn/README.md
@@ -95,6 +95,8 @@ cd DI-adventure/mario_dqn
 python3 -u mario_dqn_main.py -s <SEED> -v <VERSION> -a <ACTION SET> -o <FRAME NUMBER>
 # 以下命令的含义是，设置seed=0，游戏版本v0，动作数目为7（即SIMPLE_MOVEMENT），观测通道数目为1（即不进行叠帧）进行训练。
 python3 -u mario_dqn_main.py -s 0 -v 0 -a 7 -o 1
+
+
 ```
 训练到与环境交互3,000,000 steps时程序会自动停止，运行时长依据机器性能在3小时到10小时不等，这里如果计算资源充足的同学可以改成5,000,000 steps（main函数中设置max_env_step参数）。程序运行期间可以看看代码逻辑。
 ## 3. 智能体性能评估
@@ -170,9 +172,9 @@ python3 -u evaluate.py -ckpt <CHECKPOINT_PATH> -v <VERSION> -a <ACTION SET> -o <
 **由于同学们计算资源可能不是特别充分，这里提示一下，图像降采样、图像内容简化、叠帧、动作简化是比较有效能提升性能的方法！**
 
 以下是非常缺少计算资源和时间，最小限度需要完成的实验任务：
-1. baseline（即`v0+SIMPLE MOVEMENT+1 Frame`）跑一个seed看看结果；
-2. 尝试简化动作空间的同时进行叠帧（即`v0+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看；
-3. 观测空间去除冗余信息（即`v1+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看，如果没通关则试试换个seed；
+1. baseline（即`v0+SIMPLE MOVEMENT+1 Frame`）跑一个seed看看结果； (python3 -u mario_dqn_main.py -s 2 -v 0 -a 2 -o 1)
+2. 尝试简化动作空间的同时进行叠帧（即`v0+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看；(python3 -u mario_dqn_main.py -s 2 -v 0 -a 2 -o 4)
+3. 观测空间去除冗余信息（即`v1+[['right'], ['right', 'A']]+4 Frame`）跑一个seed看看，如果没通关则试试换个seed；  (run )
 4. 从tensorboard、可视化、CAM以及对特征空间的修改角度分析通关/没有通过的原因。
 
 对于有充足计算资源的同学，推荐增加实验的seed、延长实验步长到5M、更换其它游戏版本、尝试其它动作观测空间组合，使用其它的wrapper、以及free style；
diff --git a/mario_dqn/evaluate.py b/mario_dqn/evaluate.py
index 40c6648..f92e795 100644
--- a/mario_dqn/evaluate.py
+++ b/mario_dqn/evaluate.py
@@ -8,6 +8,7 @@ from model import DQN
 from policy import DQNPolicy
 from ding.config import compile_config
 from ding.envs import DingEnvWrapper
+import os
 import gym_super_mario_bros
 from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
 from nes_py.wrappers import JoypadSpace
@@ -28,7 +29,7 @@ def wrapped_mario_env(model, cam_video_path, version=0, action=2, obs=1):
                 lambda env: ScaledFloatFrameWrapper(env),
                 lambda env: FrameStackWrapper(env, n_frames=obs),
                 lambda env: FinalEvalRewardEnv(env),
-                lambda env: RecordCAM(env, cam_model=model, video_folder=cam_video_path)
+                lambda env: RecordCAM(env, cam_model=model, video_folder=cam_video_path, name_prefix='mario_cam_v'+str(version)+'_'+str(action)+'a_'+str(obs)+'f')
             ]
         }
     )
@@ -77,19 +78,60 @@ def evaluate(args, state_dict, seed, video_dir_path, eval_times):
         pass
 
 
+#exp map:
+     # python -u mario_dqn_main.py -s 2 -v 0 -a 7 -o 1
+    # python -u mario_dqn_main.py -s 2 -v 0 -a 2 -o 1
+    # python -u mario_dqn_main.py -s 2 -v 0 -a 7 -o 4
+    # python -u mario_dqn_main.py -s 2 -v 0 -a 2 -o 4
+    # python -u mario_dqn_main.py -s 2 -v 1 -a 2 -o 4
+
+
 if __name__ == "__main__":
     import argparse
     parser = argparse.ArgumentParser()
-    parser.add_argument("--seed", "-s", type=int, default=0)
+    parser.add_argument("--exp", "-e", type=int, default=None)
+    parser.add_argument("--seed", "-s", type=int, default=2)
     parser.add_argument("--checkpoint", "-ckpt", type=str, default='./exp/v0_1a_7f_seed0/ckpt/ckpt_best.pth.tar')
     parser.add_argument("--replay_path", "-rp", type=str, default='./eval_videos')
     parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
     parser.add_argument("--action", "-a", type=int, default=7, choices=[2,7,12])
     parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4])
     args = parser.parse_args()
+
+    if args.exp == 1 or args.exp == 71:
+        v,a,o,e = 0,7,1,1
+    elif   args.exp == 2 or args.exp == 21:
+        v,a,o,e = 0,2,1,2
+    elif   args.exp == 3 or args.exp == 74:
+        v,a,o,e = 0,7,4,3
+    elif   args.exp == 4 or args.exp == 24:
+        v,a,o,e = 0,2,4,4
+    elif   args.exp == 5 or args.exp == 124:
+        v,a,o,e = 1,2,4,5
+    elif args.exp == 6 or args.exp == 1124:
+        v,a,o,e = 1,12,4,6
+    
+    # other experiments without baseline
+    elif args.exp == 224:
+        v,a,o,e = 2,2,4,224
+    elif args.exp == 324:
+        v,a,o,e = 3,2,4,324
+    elif args.exp == 3124:
+        v,a,o,e = 3,12,4,3124
+    
+    
+    if args.exp != None:
+        args.version = v
+        args.action = a
+        args.obs = o
+        args.exp = e
+        args.checkpoint = f"~/WS/DI-adventure/mario_dqn/exp/v{args.version}_{args.action}a_{args.obs}f_seed{args.seed}/ckpt/ckpt_best.pth.tar"  
+        args.replay_path += f"/mario_exp{args.exp}_v{args.version}_{args.action}a_{args.obs}f_seed{args.seed}"
+    
     mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
     mario_dqn_config.policy.model.action_shape=args.action
-    ckpt_path = args.checkpoint
-    video_dir_path = args.replay_path
+    ckpt_path = os.path.expanduser(args.checkpoint)
+    video_dir_path = os.path.expanduser(args.replay_path)
     state_dict = torch.load(ckpt_path, map_location='cpu')
+    
     evaluate(args, state_dict=state_dict, seed=args.seed, video_dir_path=video_dir_path, eval_times=1)
diff --git a/mario_dqn/mario_dqn_config.py b/mario_dqn/mario_dqn_config.py
index d6b051b..372a16f 100644
--- a/mario_dqn/mario_dqn_config.py
+++ b/mario_dqn/mario_dqn_config.py
@@ -10,14 +10,14 @@ mario_dqn_config = dict(
     env=dict(
         # 用来收集经验（experience）的mario环境的数目
         # 请根据机器的性能自行增减
-        collector_env_num=8,
+        collector_env_num=16,
         # 用来评估智能体性能的mario环境的数目
         # 请根据机器的性能自行增减
         evaluator_env_num=8,
         # 评估轮次
         n_evaluator_episode=8,
         # 训练停止的分数（3000分可以认为通关1-1，停止训练以节省计算资源）
-        stop_value=3000
+        stop_value=3100
     ),
     policy=dict(
         # 是否使用 CUDA 加速（必要）
diff --git a/mario_dqn/mario_dqn_main.py b/mario_dqn/mario_dqn_main.py
index 3f019e1..e8ae0fe 100644
--- a/mario_dqn/mario_dqn_main.py
+++ b/mario_dqn/mario_dqn_main.py
@@ -1,22 +1,27 @@
 """
 智能体训练入口，包含训练逻辑
 """
-from tensorboardX import SummaryWriter
-from ding.config import compile_config
-from ding.worker import BaseLearner, SampleSerialCollector, InteractionSerialEvaluator, AdvancedReplayBuffer
-from ding.envs import SyncSubprocessEnvManager, DingEnvWrapper, BaseEnvManager
-from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
-    FinalEvalRewardEnv
-from policy import DQNPolicy
-from model import DQN
-from ding.utils import set_pkg_seed
-from ding.rl_utils import get_epsilon_greedy_fn
-from mario_dqn_config import mario_dqn_config
-from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
-from nes_py.wrappers import JoypadSpace
+import torch
+import gym_super_mario_bros
 from functools import partial
+from nes_py.wrappers import JoypadSpace
+from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT
+from mario_dqn_config import mario_dqn_config
+from ding.rl_utils import get_epsilon_greedy_fn
+from ding.utils import set_pkg_seed
+from model import DQN
+from policy import DQNPolicy
+from wrapper import MaxAndSkipWrapper, WarpFrameWrapper, ScaledFloatFrameWrapper, FrameStackWrapper, \
+    FinalEvalRewardEnv, CoinRewardWrapper, MushroomRewardWrapper, ExtraInfoWrapper, CoinRewardWrapper, MushroomRewardWrapper
+from ding.envs import SyncSubprocessEnvManager, DingEnvWrapper, BaseEnvManager
+from ding.worker import BaseLearner, SampleSerialCollector, InteractionSerialEvaluator, AdvancedReplayBuffer
+from ding.config import compile_config
+from tensorboardX import SummaryWriter
 import os
-import gym_super_mario_bros
+# 设置环境变量以优化 CUDA 内存使用
+os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
+# 允许 cuDNN 使用更多内存来找到最快的算法
+os.environ['CUDNN_BENCHMARK'] = '0'
 
 
 # 动作相关配置
@@ -40,15 +45,22 @@ def wrapped_mario_env(version=0, action=7, obs=1):
                 lambda env: ScaledFloatFrameWrapper(env),
                 # 默认wrapper：叠帧，将连续n_frames帧叠到一起，返回shape为(n_frames,84,84)的图片observation
                 lambda env: FrameStackWrapper(env, n_frames=obs),
+
+                # 奖励塑形：金币和蘑菇奖励
+                lambda env: CoinRewardWrapper(env),
+                lambda env: MushroomRewardWrapper(env),
+
+                # 记录额外信息用于 TensorBoard 可视化（金币、状态等）
+                lambda env: ExtraInfoWrapper(env),
+
                 # 默认wrapper：在评估一局游戏结束时返回累计的奖励，方便统计
-                lambda env: FinalEvalRewardEnv(env),
-                # 以下是你添加的wrapper
+                lambda env: FinalEvalRewardEnv(env)
             ]
         }
     )
 
 
-def main(cfg, args, seed=0, max_env_step=int(3e6)):
+def main(cfg, args, seed=0, max_env_step=int(5e6)):
     # Easydict类实例，包含一些配置
     cfg = compile_config(
         cfg,
@@ -65,12 +77,14 @@ def main(cfg, args, seed=0, max_env_step=int(3e6)):
     collector_env_num, evaluator_env_num = cfg.env.collector_env_num, cfg.env.evaluator_env_num
     # 收集经验的环境，使用并行环境管理器
     collector_env = SyncSubprocessEnvManager(
-        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(collector_env_num)], cfg=cfg.env.manager
-    )
+        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs)
+                for _ in range(collector_env_num)],
+        cfg=cfg.env.manager)
     # 评估性能的环境，使用并行环境管理器
     evaluator_env = SyncSubprocessEnvManager(
-        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs) for _ in range(evaluator_env_num)], cfg=cfg.env.manager
-    )
+        env_fn=[partial(wrapped_mario_env, version=args.version, action=args.action, obs=args.obs)
+                for _ in range(evaluator_env_num)],
+        cfg=cfg.env.manager)
 
     # 为mario环境设置种子
     collector_env.seed(seed)
@@ -78,8 +92,16 @@ def main(cfg, args, seed=0, max_env_step=int(3e6)):
     # 为torch、numpy、random等package设置种子
     set_pkg_seed(seed, use_cuda=cfg.policy.cuda)
 
+    # 设置 PyTorch 以减少内存碎片和优化 cuDNN
+    if cfg.policy.cuda and torch.cuda.is_available():
+        torch.backends.cudnn.benchmark = False  # 禁用自动调优以减少内存使用
+        torch.backends.cudnn.deterministic = True  # 使用确定性算法
+        # 清空 CUDA 缓存
+        torch.cuda.empty_cache()
+
     # 采用DQN模型
     model = DQN(**cfg.policy.model)
+
     # 采用DQN策略
     policy = DQNPolicy(cfg.policy, model=model)
 
@@ -94,8 +116,53 @@ def main(cfg, args, seed=0, max_env_step=int(3e6)):
     )
     replay_buffer = AdvancedReplayBuffer(cfg.policy.other.replay_buffer, tb_logger, exp_name=cfg.exp_name)
 
+    ckpt_path = args.resume_ckpt
+    if ckpt_path and os.path.isfile(ckpt_path):
+        print(f'\n{"="*60}')
+        print(f'加载检查点: {ckpt_path}')
+        print(f'{"="*60}')
+
+        state = torch.load(ckpt_path, map_location='cpu')
+        model.load_state_dict(state['model'])
+        print('✓ 已加载模型权重')
+
+        # 同步 target network（防止 Q 值突变）
+        # policy.learn_mode 返回的是一个包含 _target_model 的对象
+        learn_mode = policy.learn_mode
+        if hasattr(learn_mode, '_target_model'):
+            learn_mode._target_model.load_state_dict(model.state_dict())
+            print('✓ 已同步 target network')
+        else:
+            print('⚠ 未找到 target network，跳过同步')
+
+        print("\n预热 Replay Buffer（使用低探索率）...")
+        # 使用较低的 epsilon，主要利用已学策略
+        warmup_eps = 0.1  # 降低探索率，更多利用已学策略
+
+        # 收集 100 个 batch 的经验
+        for i in range(200):
+            new_data = collector.collect(n_sample=cfg.policy.collect.n_sample, policy_kwargs={'eps': warmup_eps})
+            replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)
+            if (i + 1) % 20 == 0:
+                print(f'  进度: {i+1}/100, buffer size: {replay_buffer.count()}')
+
+        print(f'✓ Buffer 预热完成: {replay_buffer.count()} samples')
+        print(f'{"="*60}\n')
+
     # 设置epsilon greedy
     eps_cfg = cfg.policy.other.eps
+
+    # 如果加载了权重，使用固定的低探索率（纯利用模式）
+    if ckpt_path and os.path.isfile(ckpt_path):
+        from easydict import EasyDict
+        eps_cfg = EasyDict(eps_cfg.copy())
+        eps_cfg.start = 0.05  # 固定为 0.05，不再衰减
+        eps_cfg.end = 0.05
+        eps_cfg.decay = 1
+        print(f'✓ Epsilon 设置为固定值 0.05（利用模式，继续训练）\n')
+    else:
+        print(f'✓ Epsilon 从 {eps_cfg.start} 开始衰减到 {eps_cfg.end}（探索模式，从头训练）\n')
+
     epsilon_greedy = get_epsilon_greedy_fn(eps_cfg.start, eps_cfg.end, eps_cfg.decay, eps_cfg.type)
 
     # 训练以及评估
@@ -128,13 +195,14 @@ if __name__ == "__main__":
     # 种子
     parser.add_argument("--seed", "-s", type=int, default=0)
     # 游戏版本，v0 v1 v2 v3 四种选择
-    parser.add_argument("--version", "-v", type=int, default=0, choices=[0,1,2,3])
+    parser.add_argument("--version", "-v", type=int, default=0, choices=[0, 1, 2, 3])
     # 动作集合种类，包含[["right"], ["right", "A"]]、SIMPLE_MOVEMENT、COMPLEX_MOVEMENT，分别对应2、7、12个动作
-    parser.add_argument("--action", "-a", type=int, default=7, choices=[2,7,12])
+    parser.add_argument("--action", "-a", type=int, default=7, choices=[2, 7, 12])
     # 观测空间叠帧数目，不叠帧或叠四帧
-    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1,4])
+    parser.add_argument("--obs", "-o", type=int, default=1, choices=[1, 4])
+    parser.add_argument("--resume_ckpt", type=str, default='', help="Path to the checkpoint to resume from")
     args = parser.parse_args()
     mario_dqn_config.exp_name = 'exp/v'+str(args.version)+'_'+str(args.action)+'a_'+str(args.obs)+'f_seed'+str(args.seed)
-    mario_dqn_config.policy.model.obs_shape=[args.obs, 84, 84]
-    mario_dqn_config.policy.model.action_shape=args.action
-    main(deepcopy(mario_dqn_config), args, seed=args.seed)
\ No newline at end of file
+    mario_dqn_config.policy.model.obs_shape = [args.obs, 84, 84]
+    mario_dqn_config.policy.model.action_shape = args.action
+    main(deepcopy(mario_dqn_config), args, seed=args.seed)
diff --git a/mario_dqn/wrapper.py b/mario_dqn/wrapper.py
index b7b1a08..0a22238 100644
--- a/mario_dqn/wrapper.py
+++ b/mario_dqn/wrapper.py
@@ -87,6 +87,83 @@ class CoinRewardWrapper(gym.Wrapper):
         self.num_coins = info['coins']
         return obs, reward, done, info
 
+# mushroom奖励wrapper
+
+
+class MushroomRewardWrapper(gym.Wrapper):
+    """
+    Overview:
+        add mushroom reward
+    Interface:
+        ``__init__``, ``step``
+    Properties:
+        - env (:obj:`gym.Env`): the environment to wrap.
+    """
+
+    def __init__(self, env: gym.Env):
+        super().__init__(env)
+        self.prev_status = 0
+        self._status_map = {'small': 0, 'tall': 1, 'fireball': 2}
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        curr_status = self._status_map.get(info.get('status'), self.prev_status)
+        if curr_status > self.prev_status:
+            reward += 50
+        self.prev_status = curr_status
+        return obs, reward, done, info
+
+
+# 记录额外信息的wrapper (用于TensorBoard可视化)
+class ExtraInfoWrapper(gym.Wrapper):
+    """
+    Overview:
+        记录金币、状态等额外信息到 episode_info，用于 TensorBoard 可视化
+    Interface:
+        ``__init__``, ``step``, ``reset``
+    Properties:
+        - env (:obj:`gym.Env`): the environment to wrap.
+    """
+
+    def __init__(self, env: gym.Env):
+        super().__init__(env)
+        self._status_map = {'small': 0, 'tall': 1, 'fireball': 2}
+        self.max_coins = 0
+        self.max_status = 0
+        self.total_coins_collected = 0
+
+    def reset(self, **kwargs):
+        self.max_coins = 0
+        self.max_status = 0
+        self.total_coins_collected = 0
+        return self.env.reset(**kwargs)
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+
+        # 记录金币信息
+        current_coins = info.get('coins', 0)
+        if current_coins > self.max_coins:
+            self.total_coins_collected += (current_coins - self.max_coins)
+            self.max_coins = current_coins
+
+        # 记录状态信息 (small=0, tall=1, fireball=2)
+        current_status = self._status_map.get(info.get('status', 'small'), 0)
+        if current_status > self.max_status:
+            self.max_status = current_status
+
+        # 在 episode 结束时，将额外信息添加到 info 中
+        if done:
+            if 'episode_info' not in info:
+                info['episode_info'] = {}
+            info['episode_info']['coins_collected'] = self.total_coins_collected
+            info['episode_info']['max_status'] = self.max_status
+            info['episode_info']['final_coins'] = current_coins
+            info['episode_info']['x_pos'] = info.get('x_pos', 0)
+            info['episode_info']['flag_get'] = int(info.get('flag_get', False))
+
+        return obs, reward, done, info
+
 
 # CAM相关，不需要了解
 def dump_arr2video(arr, video_folder):
@@ -114,7 +191,7 @@ def get_cam(img, model):
     input_tensor = torch.from_numpy(img).unsqueeze(0)
 
     # Construct the CAM object once, and then re-use it on many images:
-    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)
+    cam = GradCAM(model=model, target_layers=target_layers)
     targets = None
 
     # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.
@@ -239,4 +316,4 @@ class RecordCAM(gym.Wrapper):
         self.recorded_frames = 0
 
     def seed(self, seed: int) -> None:
-        self._env.seed(seed)
\ No newline at end of file
+        self._env.seed(seed)